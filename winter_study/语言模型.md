### 语言模型
#### 1.文本预处理


文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：
1.读入文本
2.分词建立字典，
3..将每个词映射到一个唯一的索引（index）
4..将文本从**词的序列转换为索引的序列**，方便输入模型

#### 2.时序数据的采样
1.时序数据的一个样本通常包含连续的字符。
2.有两种方式对时序数据进行采样，分别是**随机采样和相邻采样**。
##### 2.1随机采样

* 其中批量大小batch_size指每个小批量的样本数，num_steps为每个样本所包含的时间步数
* 每个样本是原始序列上任意截取的一段序列。相邻的两个随机小批量在原始序列上的位置**不一定相毗邻**
*每个batch内部的样本也不一定相邻
* 每次随机采样前都需要重新初始化隐藏状态
eg：输入一个从0到29的连续整数的人工序列。设批量大小和时间步数分别为2和6
```

X:  tensor([[18., 19., 20., 21., 22., 23.],
        [12., 13., 14., 15., 16., 17.]]) 
Y: tensor([[19., 20., 21., 22., 23., 24.],
        [13., 14., 15., 16., 17., 18.]]) 

X:  tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],
        [ 6.,  7.,  8.,  9., 10., 11.]]) 
Y: tensor([[ 1.,  2.,  3.,  4.,  5.,  6.],
        [ 7.,  8.,  9., 10., 11., 12.]]) 
```
##### 2.2相邻采样

* 令相邻的两个随机小批量在原始序列上的位置**相毗邻**（同一个批量内的样本不一定相邻
```math
e^{i\pi} + 1 = 0
```
）
* 此时就可以用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态
* 为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中**分离**出来
此时对应输出为
```

X:  tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],
        [15., 16., 17., 18., 19., 20.]]) 
Y: tensor([[ 1.,  2.,  3.,  4.,  5.,  6.],
        [16., 17., 18., 19., 20., 21.]]) 

X:  tensor([[ 6.,  7.,  8.,  9., 10., 11.],
        [21., 22., 23., 24., 25., 26.]]) 
Y: tensor([[ 7.,  8.,  9., 10., 11., 12.],
        [22., 23., 24., 25., 26., 27.]]) 
```

* * *
参考参与课堂的小罗同学绘制的原理图方便理解：
![a1d54f69ce6d4313ebac4bb041433927.png](en-resource://database/425:0)
